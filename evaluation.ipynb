{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62410b62196f4f8aa6cf45da5b78e968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/55068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a2583eaa504ef2bddcf7b671f46421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9b3f3a6a714ddd82e487a3fd4cdb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/training.py:65: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, Trainer\n",
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from training import data_preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model from the saved directory\n",
    "saved_model_path = './albert-finetuned'\n",
    "tokenizer = AlbertTokenizer.from_pretrained(saved_model_path)\n",
    "model = AlbertForSequenceClassification.from_pretrained(saved_model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ganchengguang/resume_seven_class\")\n",
    "label_mapping = {'PI': 0, 'Exp': 1, 'Sum': 2, 'Edu': 3, 'QC': 4, 'Skill': 5, 'Obj': 6, '': -1}\n",
    "train_dataset, val_dataset, test_dataset = data_preprocessing(ds)\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb1f8b086aa45f594a6613efc8829e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the test dataset\n",
    "def preprocess_function(examples):\n",
    "    texts = examples['text']\n",
    "    return tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=torch.tensor(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3208abd94c1a4029bcb45ef2eb9ffd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PI       0.92      0.89      0.91      1947\n",
      "         Exp       0.88      0.92      0.90      6189\n",
      "         Sum       0.67      0.61      0.64       970\n",
      "         Edu       0.88      0.89      0.89      1463\n",
      "          QC       0.63      0.44      0.52       160\n",
      "       Skill       0.73      0.65      0.69       750\n",
      "         Obj       0.87      0.85      0.86       322\n",
      "\n",
      "    accuracy                           0.86     11801\n",
      "   macro avg       0.80      0.75      0.77     11801\n",
      "weighted avg       0.86      0.86      0.86     11801\n",
      "\n",
      "Accuracy: 0.8583\n",
      "Precision: 0.8551\n",
      "Recall: 0.8583\n",
      "F1-Score: 0.8560\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from the saved directory\n",
    "saved_model_path = './albert-finetuned'\n",
    "tokenizer = AlbertTokenizer.from_pretrained(saved_model_path)\n",
    "model = AlbertForSequenceClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "def preprocess_function(examples):\n",
    "    texts = examples['text']\n",
    "    return tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Create a new Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Function to compute detailed metrics\n",
    "def compute_detailed_metrics(predictions, labels):\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(trainer, tokenized_ds):\n",
    "    predictions, labels, _ = trainer.predict(tokenized_ds['test'])\n",
    "    \n",
    "    metrics = compute_detailed_metrics(predictions, labels)\n",
    "    \n",
    "    print(classification_report(labels, np.argmax(predictions, axis=1), target_names=list(label_mapping.keys())[:-1]))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Perform the evaluation\n",
    "metrics = evaluate_model(trainer, {'test': tokenized_test})\n",
    "\n",
    "# Print the computed metrics\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Highly motivated Data Science engineer with a strong passionfor data and technology. My studies in artificial intelligence andstatistical analysis have equipped me to tackle real-world datachallenges with different International companies working ondifferent projects using NLP, Predictive Maintenance andComputer Vision.\n",
      "Predicted Label: Sum\n",
      "\n",
      "Input: Roukaia Khelifi\n",
      "Predicted Label: PI\n",
      "\n",
      "Input: Machine learning Deep learning Python SQL PLSQL  NLP Git/GIthub \n",
      "Predicted Label: Skill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure to put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example input sentences\n",
    "input_texts = [\n",
    "    \"Highly motivated Data Science engineer with a strong passionfor data and technology. My studies in artificial intelligence andstatistical analysis have equipped me to tackle real-world datachallenges with different International companies working ondifferent projects using NLP, Predictive Maintenance andComputer Vision.\",\n",
    "    \"Roukaia Khelifi\",\n",
    "    \"Machine learning Deep learning Python SQL PLSQL  NLP Git/GIthub \"\n",
    "]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = tokenizer(input_texts, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Get predictions from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convert logits to predicted class labels\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Convert predictions to label names\n",
    "predicted_labels = [inverse_label_mapping[p.item()] for p in predictions]\n",
    "\n",
    "# Print out the results\n",
    "for text, label in zip(input_texts, predicted_labels):\n",
    "    print(f\"Input: {text}\\nPredicted Label: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model from an extracted PDF Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roukaia Khelifi\n",
      "Data Scientist\n",
      "About me\n",
      "Accomplished Projects\n",
      "AchievementsHighly motivated Data Science engineer with a strong passion\n",
      "for data and technology. My studies in artificial intelligence and\n",
      "statistical analysis have equipped me to tackle real-world data\n",
      "challenges with different International companies working on\n",
      "different projects using NLP, Predictive Maintenance and\n",
      "Computer Vision.\n",
      "Technical Skills:\n",
      "Machine learning\n",
      "Deep learning\n",
      "Python\n",
      "SQL / PLSQL\n",
      "NLP\n",
      "Git/GIthub\n",
      "LLMs\n",
      "Education Background\n",
      "The Private Higher School of\n",
      "Engineering and Technology, ESPRIT\n",
      "Computer Science\n",
      "September 2019 - July 2024\n",
      "Khaireddine Pacha Ariana High SchoolMy Contact\n",
      "roukaia70@gmail.com\n",
      "Ariana, Tunis+216 29 043 930\n",
      "github.com/RoukaiaKHELIFI\n",
      "Languages\n",
      "English (Native)\n",
      "French (B2 - Intermediate)www.linkedin.com/in/roukaia-\n",
      "khelifi-046365205/\n",
      "Hobbies\n",
      "Continuous Learning: Reading,\n",
      "Studying, Exploring New Topics, and\n",
      "Documenting.\n",
      "Print Design: Adobe Photoshop\n",
      "(Flyers, Posters, Brochures, Business\n",
      "Cards, Logos)\n",
      "Video Editing: Sony VegasHigh School Graduation  2019\n",
      "March\n",
      "2023Stanford Online (Coursera)\n",
      "Machine Learning SpecializationSeptember\n",
      "2023DeepLearning.AI (Coursera)\n",
      "Generative AI with Large Language Models (LLMs)\n",
      "September\n",
      "2023University of California, Davis (Coursera)\n",
      "SQL  for Data SciencePresentation and\n",
      "Communication\n",
      "Proficient in Oral and Written\n",
      "EnglishSoft Skills:\n",
      "Developed and implemented a comprehensive predictive\n",
      "maintenance project in industrial settings.\n",
      "Leveraged data analytics and machine learning\n",
      "algorithms to anticipate equipment failures, optimize\n",
      "maintenance schedules, and reduce downtime.\n",
      "Utilized Tools: Python, TensorFlow, Pandas, Scikit-learn, SQL,\n",
      "Machine Learning.Data Science Internship\n",
      "Company: SAGEMCOM\n",
      "Location: Tunis, Tunisia (Onsite)\n",
      "Duration: Juin 2023 - August 2023Implemented advanced Natural Language Processing\n",
      "(NLP) techniques to optimize project risk management\n",
      "processes significantly.\n",
      " Developed a  Deep Learning model that responds adeptly\n",
      "to various project use cases, integrating project risk\n",
      "management techniques from the PMI and PMBOK\n",
      "corpuses.\n",
      "Utilized Tools: Python, Numpy, Pandas, PYpdf, pymupdf, NLP,\n",
      "KeyBert, Sbert, LLMs, YoloV6,Rebel, imageBB , Regex, Deep\n",
      "Learning, Networkx.NLP Internship\n",
      "Company: ESPRIT\n",
      "Location: Tunis, Tunisia (Onsite)\n",
      "Duration: September 2023 - November 2023Implemented an advanced LLM Chatbot designed to\n",
      "facilitate document reading. Instead of manually going\n",
      "through numerous documents, users can simply upload\n",
      "them to the chatbot and ask questions.\n",
      " Developed a tool that aids in automating network\n",
      "optimization, resulting in reduction in manual network\n",
      "optimization time and a decreased likelihood of errors.\n",
      "Utilized Tools: Python, Numpy, Pandas, Streamlit, LLM, LLAMA2.Netwrok Optimization Automation Internship\n",
      "Company: HUAWEI TECHNOLOGIES\n",
      "Location: Tunis, Tunisia (Onsite)\n",
      "Duration: January 2024 - June 2024\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = 'Roukaia Khelifi Cv.pdf'\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(resume_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roukaia Khelifi\n",
      "Data Scientist\n",
      "About me\n",
      "Accomplished Projects\n",
      "AchievementsHighly motivated Data Science engineer with a strong passion\n",
      "for data and technology. My studies in artificial intelligence and\n",
      "statistical analysis have equipped me to tackle real-world data\n",
      "challenges with different International companies working on\n",
      "different projects using NLP, Predictive Maintenance and\n",
      "Computer Vision.\n",
      "Technical Skills:\n",
      "Machine learning\n",
      "Deep learning\n",
      "Python\n",
      "SQL / PLSQL\n",
      "NLP\n",
      "Git/GIthub\n",
      "LLMs\n",
      "Education Background\n",
      "The Private Higher School of\n",
      "Engineering and Technology, ESPRIT\n",
      "Computer Science\n",
      "September 2019 - July 2024\n",
      "Khaireddine Pacha Ariana High SchoolMy Contact\n",
      "roukaia70@gmail.com\n",
      "Ariana, Tunis+216 29 043 930\n",
      "github.com/RoukaiaKHELIFI\n",
      "Languages\n",
      "English (Native)\n",
      "French (B2 - Intermediate)www.linkedin.com/in/roukaia-\n",
      "khelifi-046365205/\n",
      "Hobbies\n",
      "Continuous Learning: Reading,\n",
      "Studying, Exploring New Topics, and\n",
      "Documenting.\n",
      "Print Design: Adobe Photoshop\n",
      "(Flyers, Posters, Brochures, Business\n",
      "Cards, Logos)\n",
      "Video Editing: Sony VegasHigh School Graduation  2019\n",
      "March\n",
      "2023Stanford Online (Coursera)\n",
      "Machine Learning SpecializationSeptember\n",
      "2023DeepLearning.AI (Coursera)\n",
      "Generative AI with Large Language Models (LLMs)\n",
      "September\n",
      "2023University of California, Davis (Coursera)\n",
      "SQL  for Data SciencePresentation and\n",
      "Communication\n",
      "Proficient in Oral and Written\n",
      "EnglishSoft Skills:\n",
      "Developed and implemented a comprehensive predictive\n",
      "maintenance project in industrial settings.\n",
      "Leveraged data analytics and machine learning\n",
      "algorithms to anticipate equipment failures, optimize\n",
      "maintenance schedules, and reduce downtime.\n",
      "Utilized Tools: Python, TensorFlow, Pandas, Scikit-learn, SQL,\n",
      "Machine Learning.Data Science Internship\n",
      "Company: SAGEMCOM\n",
      "Location: Tunis, Tunisia (Onsite)\n",
      "Duration: Juin 2023 - August 2023Implemented advanced Natural Language Processing\n",
      "(NLP) techniques to optimize project risk management\n",
      "processes significantly.\n",
      "Developed a  Deep Learning model that responds adeptly\n",
      "to various project use cases, integrating project risk\n",
      "management techniques from the PMI and PMBOK\n",
      "corpuses.\n",
      "Utilized Tools: Python, Numpy, Pandas, PYpdf, pymupdf, NLP,\n",
      "KeyBert, Sbert, LLMs, YoloV6,Rebel, imageBB , Regex, Deep\n",
      "Learning, Networkx.NLP Internship\n",
      "Company: ESPRIT\n",
      "Location: Tunis, Tunisia (Onsite)\n",
      "Duration: September 2023 - November 2023Implemented an advanced LLM Chatbot designed to\n",
      "facilitate document reading. Instead of manually going\n",
      "through numerous documents, users can simply upload\n",
      "them to the chatbot and ask questions.\n",
      "Developed a tool that aids in automating network\n",
      "optimization, resulting in reduction in manual network\n",
      "optimization time and a decreased likelihood of errors.\n",
      "Utilized Tools: Python, Numpy, Pandas, Streamlit, LLM, LLAMA2.Netwrok Optimization Automation Internship\n",
      "Company: HUAWEI TECHNOLOGIES\n",
      "Location: Tunis, Tunisia (Onsite)\n",
      "Duration: January 2024 - June 2024\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split the text into sentences or lines\n",
    "resume_lines = resume_text.splitlines()\n",
    "\n",
    "# Clean up empty lines or unnecessary spaces\n",
    "resume_lines = [line.strip() for line in resume_lines if line.strip()]\n",
    "\n",
    "# Print the lines\n",
    "for line in resume_lines:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: Roukaia Khelifi\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Data Scientist\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: About me\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Accomplished Projects\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: AchievementsHighly motivated Data Science engineer with a strong passion\n",
      "Predicted Label: Sum\n",
      "\n",
      "Line: for data and technology. My studies in artificial intelligence and\n",
      "Predicted Label: Sum\n",
      "\n",
      "Line: statistical analysis have equipped me to tackle real-world data\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: challenges with different International companies working on\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: different projects using NLP, Predictive Maintenance and\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Computer Vision.\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: Technical Skills:\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: Machine learning\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: Deep learning\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: Python\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: SQL / PLSQL\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: NLP\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: Git/GIthub\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: LLMs\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: Education Background\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: The Private Higher School of\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: Engineering and Technology, ESPRIT\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: Computer Science\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: September 2019 - July 2024\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Khaireddine Pacha Ariana High SchoolMy Contact\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: roukaia70@gmail.com\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Ariana, Tunis+216 29 043 930\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: github.com/RoukaiaKHELIFI\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Languages\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: English (Native)\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: French (B2 - Intermediate)www.linkedin.com/in/roukaia-\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: khelifi-046365205/\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Hobbies\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Continuous Learning: Reading,\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Studying, Exploring New Topics, and\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Documenting.\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Print Design: Adobe Photoshop\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: (Flyers, Posters, Brochures, Business\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Cards, Logos)\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Video Editing: Sony VegasHigh School Graduation  2019\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: March\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: 2023Stanford Online (Coursera)\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Machine Learning SpecializationSeptember\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: 2023DeepLearning.AI (Coursera)\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: Generative AI with Large Language Models (LLMs)\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: September\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: 2023University of California, Davis (Coursera)\n",
      "Predicted Label: Edu\n",
      "\n",
      "Line: SQL  for Data SciencePresentation and\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Communication\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Proficient in Oral and Written\n",
      "Predicted Label: Sum\n",
      "\n",
      "Line: EnglishSoft Skills:\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: Developed and implemented a comprehensive predictive\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: maintenance project in industrial settings.\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Leveraged data analytics and machine learning\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: algorithms to anticipate equipment failures, optimize\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: maintenance schedules, and reduce downtime.\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Utilized Tools: Python, TensorFlow, Pandas, Scikit-learn, SQL,\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Machine Learning.Data Science Internship\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Company: SAGEMCOM\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Location: Tunis, Tunisia (Onsite)\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Duration: Juin 2023 - August 2023Implemented advanced Natural Language Processing\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: (NLP) techniques to optimize project risk management\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: processes significantly.\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Developed a  Deep Learning model that responds adeptly\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: to various project use cases, integrating project risk\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: management techniques from the PMI and PMBOK\n",
      "Predicted Label: QC\n",
      "\n",
      "Line: corpuses.\n",
      "Predicted Label: PI\n",
      "\n",
      "Line: Utilized Tools: Python, Numpy, Pandas, PYpdf, pymupdf, NLP,\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: KeyBert, Sbert, LLMs, YoloV6,Rebel, imageBB , Regex, Deep\n",
      "Predicted Label: Skill\n",
      "\n",
      "Line: Learning, Networkx.NLP Internship\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Company: ESPRIT\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Location: Tunis, Tunisia (Onsite)\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Duration: September 2023 - November 2023Implemented an advanced LLM Chatbot designed to\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: facilitate document reading. Instead of manually going\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: through numerous documents, users can simply upload\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: them to the chatbot and ask questions.\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Developed a tool that aids in automating network\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: optimization, resulting in reduction in manual network\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: optimization time and a decreased likelihood of errors.\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Utilized Tools: Python, Numpy, Pandas, Streamlit, LLM, LLAMA2.Netwrok Optimization Automation Internship\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Company: HUAWEI TECHNOLOGIES\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Location: Tunis, Tunisia (Onsite)\n",
      "Predicted Label: Exp\n",
      "\n",
      "Line: Duration: January 2024 - June 2024\n",
      "Predicted Label: Exp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and classify each line\n",
    "for line in resume_lines:\n",
    "    inputs = tokenizer(line, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to predicted class label\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = inverse_label_mapping[prediction]\n",
    "\n",
    "    # Print the line with its predicted label\n",
    "    print(f\"Line: {line}\\nPredicted Label: {predicted_label}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
